# 导入必要库
from datasets import load_dataset
from evaluate import load
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
import torch
import os
import numpy as np  # 用于数组处理

# 确认GPU可用
print("使用GPU训练:", torch.cuda.is_available())
print("当前GPU:", torch.cuda.get_device_name(0))

# 1. 加载并预处理数据集（确保标签正确）
data_dir = os.path.abspath(r"C:\Users\陈可挺\Desktop\glue\glue\SST-2")
dataset = load_dataset(
    "csv",
    data_files={
        "train": os.path.join(data_dir, "train.tsv"),
        "validation": os.path.join(data_dir, "dev.tsv")
    },
    delimiter="\t",
    column_names=["sentence", "label"],
    skiprows=1
)


# 强制标签为整数并确保无缺失值
def clean_labels(example):
    try:
        return {"label": int(example["label"])}
    except:
        # 处理异常标签（如空值或非数字）
        return {"label": 0}  # 临时赋值，实际应检查数据


dataset = dataset.map(clean_labels)
# 过滤可能的异常样本（如标签不在0/1范围内）
dataset = dataset.filter(lambda x: x["label"] in [0, 1])

# 2. 加载模型和分词器
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)


# 3. 数据预处理
def preprocess_function(examples):
    return tokenizer(examples["sentence"], truncation=True, max_length=512, padding=False)


tokenized_datasets = dataset.map(preprocess_function, batched=True)

# 4. 动态填充器
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 5. 评估指标（彻底修正形状和类型问题）
metric = load("glue", "sst2")


def compute_metrics(eval_pred):
    logits, labels = eval_pred

    #将logits和labels转换为NumPy数组（确保脱离计算图）
    logits = logits.cpu().numpy() if isinstance(logits, torch.Tensor) else logits
    labels = labels.cpu().numpy() if isinstance(labels, torch.Tensor) else labels

    #确保labels是1D数组（展平所有维度）
    labels = np.squeeze(labels).astype(int)  # 强制整数类型

    #计算预测并确保形状为1D
    predictions = np.argmax(logits, axis=-1).flatten()  # 展平为1D

    #确保两者长度一致（过滤可能的异常样本）
    min_len = min(len(predictions), len(labels))
    predictions = predictions[:min_len]
    labels = labels[:min_len]

    # 打印调试信息
    print(f"预测形状: {predictions.shape}, 标签形状: {labels.shape}")

    # 计算准确率
    correct = (predictions == labels).sum()
    accuracy = correct / len(predictions)
    return {"accuracy": float(accuracy)}


# 6. 训练参数
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=True,
    logging_dir="./logs",
    logging_steps=100,
)

# 7. 训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)

print("开始训练...")
trainer.train()

# 最终评估
eval_results = trainer.evaluate()
print(f"\n最终验证集准确率: {eval_results['eval_accuracy']:.4f}")
